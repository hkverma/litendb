{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TPCH Query 6 and Query 5 runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12.3\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/20 12:17:54 WARN Utils: Your hostname, flexdevbox resolves to a loopback address: 127.0.1.1; using 192.168.1.203 instead (on interface wlo1)\n",
      "25/10/20 12:17:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/20 12:17:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.range(1000).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists in ./tpch-sf1g/, skipping download.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "#tpch_dataset = 'tpch-sfdot001g' # for sf=0.001g \n",
    "tpch_dataset = 'tpch-sf1g' # for sf=1g\n",
    "local_dir=f'./{tpch_dataset}/'\n",
    "if not os.path.exists(local_dir):\n",
    "    print(f'Downloading dataset into {local_dir} ...')\n",
    "    snapshot_download(\n",
    "        repo_id=\"hkverma/\"+tpch_dataset,\n",
    "        repo_type=\"dataset\",\n",
    "        local_dir=local_dir,\n",
    "        local_dir_use_symlinks=False  # ensures real copies, not symlinks\n",
    "    )\n",
    "    print('Download Complete.')\n",
    "else:\n",
    "    print(f\"Dataset already exists in {local_dir}, skipping download.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('L_ORDERKEY', IntegerType(), True), StructField('L_PARTKEY', IntegerType(), True), StructField('L_SUPPKEY', IntegerType(), True), StructField('L_LINENUMBER', IntegerType(), True), StructField('L_QUANTITY', IntegerType(), True), StructField('L_EXTENDEDPRICE', DoubleType(), True), StructField('L_DISCOUNT', DoubleType(), True), StructField('L_TAX', DoubleType(), True), StructField('L_RETURNFLAG', StringType(), True), StructField('L_LINESTATUS', StringType(), True), StructField('L_SHIPDATE', DateType(), True), StructField('L_COMMITDATE', DateType(), True), StructField('L_RECEIPTDATE', DateType(), True), StructField('L_SHIPINSTRUCT', StringType(), True), StructField('L_SHIPMODE', StringType(), True), StructField('L_COMMENT', StringType(), True)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = StructType([StructField('L_ORDERKEY',IntegerType(),True),\n",
    "                     StructField('L_PARTKEY',IntegerType(),True),\n",
    "                     StructField('L_SUPPKEY',IntegerType(),True),\n",
    "                     StructField('L_LINENUMBER',IntegerType(),True),\n",
    "                     StructField('L_QUANTITY',IntegerType(),True),\n",
    "                     StructField('L_EXTENDEDPRICE',DoubleType(),True),\n",
    "                     StructField('L_DISCOUNT',DoubleType(),True),\n",
    "                     StructField('L_TAX',DoubleType(),True),\n",
    "                     StructField('L_RETURNFLAG',StringType(),True),\n",
    "                     StructField('L_LINESTATUS',StringType(),True),\n",
    "                     StructField('L_SHIPDATE',DateType(),True),\n",
    "                     StructField('L_COMMITDATE',DateType(),True),\n",
    "                     StructField('L_RECEIPTDATE',DateType(),True),\n",
    "                     StructField('L_SHIPINSTRUCT',StringType(),True),\n",
    "                     StructField('L_SHIPMODE',StringType(),True),\n",
    "                     StructField('L_COMMENT',StringType(),True)])\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- L_ORDERKEY: integer (nullable = true)\n",
      " |-- L_PARTKEY: integer (nullable = true)\n",
      " |-- L_SUPPKEY: integer (nullable = true)\n",
      " |-- L_LINENUMBER: integer (nullable = true)\n",
      " |-- L_QUANTITY: integer (nullable = true)\n",
      " |-- L_EXTENDEDPRICE: double (nullable = true)\n",
      " |-- L_DISCOUNT: double (nullable = true)\n",
      " |-- L_TAX: double (nullable = true)\n",
      " |-- L_RETURNFLAG: string (nullable = true)\n",
      " |-- L_LINESTATUS: string (nullable = true)\n",
      " |-- L_SHIPDATE: date (nullable = true)\n",
      " |-- L_COMMITDATE: date (nullable = true)\n",
      " |-- L_RECEIPTDATE: date (nullable = true)\n",
      " |-- L_SHIPINSTRUCT: string (nullable = true)\n",
      " |-- L_SHIPMODE: string (nullable = true)\n",
      " |-- L_COMMENT: string (nullable = true)\n",
      "\n",
      "+----------+---------+---------+------------+----------+---------------+----------+-----+------------+------------+----------+------------+-------------+-----------------+----------+--------------------+\n",
      "|L_ORDERKEY|L_PARTKEY|L_SUPPKEY|L_LINENUMBER|L_QUANTITY|L_EXTENDEDPRICE|L_DISCOUNT|L_TAX|L_RETURNFLAG|L_LINESTATUS|L_SHIPDATE|L_COMMITDATE|L_RECEIPTDATE|   L_SHIPINSTRUCT|L_SHIPMODE|           L_COMMENT|\n",
      "+----------+---------+---------+------------+----------+---------------+----------+-----+------------+------------+----------+------------+-------------+-----------------+----------+--------------------+\n",
      "|         1|   155190|     7706|           1|        17|       21168.23|      0.04| 0.02|           N|           O|1996-03-13|  1996-02-12|   1996-03-22|DELIVER IN PERSON|     TRUCK|egular courts abo...|\n",
      "|         1|    67310|     7311|           2|        36|       45983.16|      0.09| 0.06|           N|           O|1996-04-12|  1996-02-28|   1996-04-20| TAKE BACK RETURN|      MAIL|ly final dependen...|\n",
      "|         1|    63700|     3701|           3|         8|        13309.6|       0.1| 0.02|           N|           O|1996-01-29|  1996-03-05|   1996-01-31| TAKE BACK RETURN|   REG AIR|riously. regular,...|\n",
      "|         1|     2132|     4633|           4|        28|       28955.64|      0.09| 0.06|           N|           O|1996-04-21|  1996-03-30|   1996-05-16|             NONE|       AIR|lites. fluffily e...|\n",
      "|         1|    24027|     1534|           5|        24|       22824.48|       0.1| 0.04|           N|           O|1996-03-30|  1996-03-14|   1996-04-01|             NONE|       FOB| pending foxes. s...|\n",
      "+----------+---------+---------+------------+----------+---------------+----------+-----+------------+------------+----------+------------+-------------+-----------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lineitemDf = spark.read.format('csv').options(header='true').options(delimiter='|').schema(schema).load(f\"{local_dir}/lineitem.csv\")\n",
    "lineitemDf.printSchema()\n",
    "lineitemDf.show(5)\n",
    "lineitemDf.createOrReplaceTempView(\"lineitem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                        (0 + 20) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             revenue|\n",
      "+--------------------+\n",
      "|1.5659409560960007E8|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sqlDf = spark.sql(\"select sum(l_extendedprice * l_discount) as revenue from lineitem \"\n",
    "                  \"where l_shipdate >= date '1997-01-01' \"\n",
    "                  \"and l_shipdate < date '1997-01-01' + interval '1' year \"\n",
    "                  \" and l_discount between 0.07 - 0.01 and 0.07 + 0.01 \"\n",
    "                  \" and l_quantity < 25;\")\n",
    "sqlDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following neews to be done for the sql code shown below. load customer, orders, lineitem, supplier, nation, region tables. Then run query 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- C_CUSTKEY: integer (nullable = true)\n",
      " |-- C_NAME: string (nullable = true)\n",
      " |-- C_ADDRESS: string (nullable = true)\n",
      " |-- C_NATIONKEY: integer (nullable = true)\n",
      " |-- C_PHONE: string (nullable = true)\n",
      " |-- C_ACCTBAL: double (nullable = true)\n",
      " |-- C_MKTSEGMENT: string (nullable = true)\n",
      " |-- C_COMMENT: string (nullable = true)\n",
      "\n",
      "+---------+------------------+--------------------+-----------+---------------+---------+------------+--------------------+\n",
      "|C_CUSTKEY|            C_NAME|           C_ADDRESS|C_NATIONKEY|        C_PHONE|C_ACCTBAL|C_MKTSEGMENT|           C_COMMENT|\n",
      "+---------+------------------+--------------------+-----------+---------------+---------+------------+--------------------+\n",
      "|        1|Customer#000000001|   IVhzIApeRb ot,c,E|         15|25-989-741-2988|   711.56|    BUILDING|to the even, regu...|\n",
      "|        2|Customer#000000002|XSTf4,NCwDVaWNe6t...|         13|23-768-687-3665|   121.65|  AUTOMOBILE|l accounts. blith...|\n",
      "|        3|Customer#000000003|        MG9kdTD2WBHm|          1|11-719-748-3364|  7498.12|  AUTOMOBILE| deposits eat sly...|\n",
      "|        4|Customer#000000004|         XxVSJsLAGtn|          4|14-128-190-5944|  2866.83|   MACHINERY| requests. final,...|\n",
      "|        5|Customer#000000005|KvpyuHCplrB84WgAi...|          3|13-750-942-6364|   794.47|   HOUSEHOLD|n accounts will h...|\n",
      "+---------+------------------+--------------------+-----------+---------------+---------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf = spark.read.format('csv').options(header='true').options(delimiter='|').options(inferSchema='true').load(f\"{local_dir}/customer.csv\")\n",
    "customerDf.printSchema()\n",
    "customerDf.show(5)\n",
    "customerDf.createOrReplaceTempView(\"customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- O_ORDERKEY: integer (nullable = true)\n",
      " |-- O_CUSTKEY: integer (nullable = true)\n",
      " |-- O_ORDERSTATUS: string (nullable = true)\n",
      " |-- O_TOTALPRICE: double (nullable = true)\n",
      " |-- O_ORDERDATE: date (nullable = true)\n",
      " |-- O_ORDERPRIORITY: string (nullable = true)\n",
      " |-- O_CLERK: string (nullable = true)\n",
      " |-- O_SHIPPRIORITY: integer (nullable = true)\n",
      " |-- O_COMMENT: string (nullable = true)\n",
      "\n",
      "+----------+---------+-------------+------------+-----------+---------------+---------------+--------------+--------------------+\n",
      "|O_ORDERKEY|O_CUSTKEY|O_ORDERSTATUS|O_TOTALPRICE|O_ORDERDATE|O_ORDERPRIORITY|        O_CLERK|O_SHIPPRIORITY|           O_COMMENT|\n",
      "+----------+---------+-------------+------------+-----------+---------------+---------------+--------------+--------------------+\n",
      "|         1|    36901|            O|   173665.47| 1996-01-02|          5-LOW|Clerk#000000951|             0|nstructions sleep...|\n",
      "|         2|    78002|            O|    46929.18| 1996-12-01|       1-URGENT|Clerk#000000880|             0| foxes. pending a...|\n",
      "|         3|   123314|            F|   193846.25| 1993-10-14|          5-LOW|Clerk#000000955|             0|sly final account...|\n",
      "|         4|   136777|            O|    32151.78| 1995-10-11|          5-LOW|Clerk#000000124|             0|sits. slyly regul...|\n",
      "|         5|    44485|            F|    144659.2| 1994-07-30|          5-LOW|Clerk#000000925|             0|quickly. bold dep...|\n",
      "+----------+---------+-------------+------------+-----------+---------------+---------------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ordersDf = spark.read.format('csv').options(header='true').options(delimiter='|').options(inferSchema='true').load(f\"{local_dir}/orders.csv\")\n",
    "ordersDf.printSchema()\n",
    "ordersDf.show(5)\n",
    "ordersDf.createOrReplaceTempView(\"orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- S_SUPPKEY: integer (nullable = true)\n",
      " |-- S_NAME: string (nullable = true)\n",
      " |-- S_ADDRESS: string (nullable = true)\n",
      " |-- S_NATIONKEY: integer (nullable = true)\n",
      " |-- S_PHONE: string (nullable = true)\n",
      " |-- S_ACCTBAL: double (nullable = true)\n",
      " |-- S_COMMENT: string (nullable = true)\n",
      "\n",
      "+---------+------------------+--------------------+-----------+---------------+---------+--------------------+\n",
      "|S_SUPPKEY|            S_NAME|           S_ADDRESS|S_NATIONKEY|        S_PHONE|S_ACCTBAL|           S_COMMENT|\n",
      "+---------+------------------+--------------------+-----------+---------------+---------+--------------------+\n",
      "|        1|Supplier#000000001| N kD4on9OM Ipw3,...|         17|27-918-335-1736|  5755.94|each slyly above ...|\n",
      "|        2|Supplier#000000002|89eJ5ksX3ImxJQBvx...|          5|15-679-861-2259|  4032.68| slyly bold instr...|\n",
      "|        3|Supplier#000000003|q1,G3Pj6OjIuUYfUo...|          1|11-383-516-1199|   4192.4|blithely silent r...|\n",
      "|        4|Supplier#000000004|Bk7ah4CK8SYQTepEm...|         15|25-843-787-7479|  4641.08|riously even requ...|\n",
      "|        5|Supplier#000000005|   Gcdm2rJRzl5qlTVzc|         11|21-151-690-3663|  -283.84|. slyly regular p...|\n",
      "+---------+------------------+--------------------+-----------+---------------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "supplierDf = spark.read.format('csv').options(header='true').options(delimiter='|').options(inferSchema='true').load(f\"{local_dir}/supplier.csv\")\n",
    "supplierDf.printSchema()\n",
    "supplierDf.show(5)\n",
    "supplierDf.createOrReplaceTempView(\"supplier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- N_NATIONKEY: integer (nullable = true)\n",
      " |-- N_NAME: string (nullable = true)\n",
      " |-- N_REGIONKEY: integer (nullable = true)\n",
      " |-- N_COMMENT: string (nullable = true)\n",
      "\n",
      "+-----------+---------+-----------+--------------------+\n",
      "|N_NATIONKEY|   N_NAME|N_REGIONKEY|           N_COMMENT|\n",
      "+-----------+---------+-----------+--------------------+\n",
      "|          0|  ALGERIA|          0| haggle. carefull...|\n",
      "|          1|ARGENTINA|          1|al foxes promise ...|\n",
      "|          2|   BRAZIL|          1|y alongside of th...|\n",
      "|          3|   CANADA|          1|eas hang ironic, ...|\n",
      "|          4|    EGYPT|          4|y above the caref...|\n",
      "+-----------+---------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nationDf = spark.read.format('csv').options(header='true').options(delimiter='|').options(inferSchema='true').load(f\"{local_dir}/nation.csv\")\n",
    "nationDf.printSchema()\n",
    "nationDf.show(5)\n",
    "nationDf.createOrReplaceTempView(\"nation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- R_REGIONKEY: integer (nullable = true)\n",
      " |-- R_NAME: string (nullable = true)\n",
      " |-- R_COMMENT: string (nullable = true)\n",
      "\n",
      "+-----------+-----------+--------------------+\n",
      "|R_REGIONKEY|     R_NAME|           R_COMMENT|\n",
      "+-----------+-----------+--------------------+\n",
      "|          0|     AFRICA|lar deposits. bli...|\n",
      "|          1|    AMERICA|hs use ironic, ev...|\n",
      "|          2|       ASIA|ges. thinly even ...|\n",
      "|          3|     EUROPE|ly final courts c...|\n",
      "|          4|MIDDLE EAST|uickly special ac...|\n",
      "+-----------+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regionDf = spark.read.format('csv').options(header='true').options(delimiter='|').options(inferSchema='true').load(f\"{local_dir}/region.csv\")\n",
    "regionDf.printSchema()\n",
    "regionDf.show(5)\n",
    "regionDf.createOrReplaceTempView(\"region\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+\n",
      "|        n_name|            revenue|\n",
      "+--------------+-------------------+\n",
      "|        RUSSIA|     5.6063022839E7|\n",
      "|       ROMANIA|    5.49946485948E7|\n",
      "|UNITED KINGDOM|     5.4686147489E7|\n",
      "|        FRANCE|5.194113723340001E7|\n",
      "|       GERMANY|    5.15364981838E7|\n",
      "+--------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql5Df = spark.sql(\"select n_name, sum(l_extendedprice * (1 - l_discount)) as revenue \"\n",
    "    \"from customer, orders, lineitem, supplier, nation, region \"\n",
    "    \"where c_custkey = o_custkey \"\n",
    "    \"and l_orderkey = o_orderkey \"\n",
    "    \"and l_suppkey = s_suppkey \"\n",
    "    \"and c_nationkey = s_nationkey \"\n",
    "    \"and s_nationkey = n_nationkey \"\n",
    "    \"and n_regionkey = r_regionkey \"\n",
    "    \"and r_name = 'EUROPE' \"\n",
    "    \"and o_orderdate >= date '1995-01-01' \"\n",
    "    \"and o_orderdate < date '1995-01-01' + interval '1' year \"\n",
    "    \"group by n_name order by revenue desc\")\n",
    "sql5Df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
