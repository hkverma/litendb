{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b4578d48b219735043a4d2102119fb307d2fc83f"
   },
   "source": [
    "# New York City Taxi Fare Prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Training and Prediction in Liten Cache\n",
    "* Liten database can serve in batch and maintain versions using a multi-version control system\n",
    "* It integrates data from different sources that Data Science/ML engineer need not worry about\n",
    "* Cognitive Services - Trained models performing predictive actions for target markets\n",
    "* ML Operations \n",
    "    * Data serving in batches\n",
    "    * Model, Features, Embeddings versions and registering\n",
    "    * Deployment with single statements on different clouds\n",
    "\n",
    "XG Boost example here shows the following\n",
    "* train using XGBoost in pandas\n",
    "* predict using models in liten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import pandas, xgboost, arrow and liten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pyarrow as pa\n",
    "from pyarrow import csv\n",
    "import litendb as ten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fb969a26e52931bcaced3cbb7a36d8d8b1b04556"
   },
   "source": [
    "## Data cleaning & Feature engineering Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cabf715bd2a7afd19a26a3af4f043abb8f5877f9"
   },
   "outputs": [],
   "source": [
    "def remove_outliers(df):\n",
    "    df = df.dropna()\n",
    "    mask = df['fare_amount'].between(0, 500)\n",
    "    mask &= df['passenger_count'].between(0, 6)\n",
    "\n",
    "    # to select coordinates: https://www.openstreetmap.org/export\n",
    "    mask &= df['pickup_longitude'].between(-75, -73)\n",
    "    mask &= df['dropoff_longitude'].between(-75, -73)\n",
    "    mask &= df['pickup_latitude'].between(40, 42)\n",
    "    mask &= df['dropoff_latitude'].between(40, 42)\n",
    "\n",
    "    return df[mask]\n",
    "\n",
    "def manhattan(pickup, dropoff):\n",
    "    pickup_long, pickup_lat = pickup\n",
    "    dropoff_long, dropoff_lat = dropoff\n",
    "    return np.abs(dropoff_long - pickup_long) + np.abs(dropoff_lat - pickup_lat)\n",
    "\n",
    "def extract_distance_features(df):\n",
    "    df['abs_diff_longitude'] = (df['dropoff_longitude'] - df['pickup_longitude']).abs()\n",
    "    df['abs_diff_latitude'] = (df['dropoff_latitude'] - df['pickup_latitude']).abs()\n",
    "\n",
    "    pickup = (df['pickup_longitude'], df['pickup_latitude'])\n",
    "    dropoff = (df['dropoff_longitude'], df['dropoff_latitude'])\n",
    "    df['distance'] = manhattan(pickup, dropoff)\n",
    "\n",
    "    # Distances to nearby airports, and city center\n",
    "    # https://www.kaggle.com/btyuhas/bayesian-optimization-with-xgboost\n",
    "    coordinates = {\n",
    "        'nyc': (-74.0063889, 40.7141667),\n",
    "        'jfk': (-73.7822222222, 40.6441666667),\n",
    "        'ewr': (-74.175, 40.69),\n",
    "        'lgr': (-73.87, 40.77)\n",
    "    }\n",
    "\n",
    "    for name, coord in coordinates.items():\n",
    "        df[f'pickup_distance_to_{name}'] = manhattan(coord, pickup)\n",
    "        df[f'dropoff_distance_to_{name}'] = manhattan(coord, dropoff)\n",
    "\n",
    "    return df\n",
    "\n",
    "def extract_datetime_features(df):\n",
    "    # Removing unecessary information from the datetime string\n",
    "    # https://www.kaggle.com/btyuhas/bayesian-optimization-with-xgboost\n",
    "    pickup_datetime = df['pickup_datetime'].str.slice(0, 16)\n",
    "    pickup_datetime = pd.to_datetime(pickup_datetime, utc=True, format='%Y-%m-%d %H:%M')\n",
    "\n",
    "    df['year'] = pickup_datetime.dt.year\n",
    "    df['month'] = pickup_datetime.dt.month\n",
    "    df['day'] = pickup_datetime.dt.day\n",
    "    df['dayofweek'] = pickup_datetime.dt.dayofweek\n",
    "    df['hour'] = pickup_datetime.dt.hour\n",
    "\n",
    "    return df.drop(columns='pickup_datetime')\n",
    "\n",
    "def extract_features(df):\n",
    "    df = extract_distance_features(df)\n",
    "    df = extract_datetime_features(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add arrow csv to liten cache\n",
    "Create a new cache, add NY Taxi data to cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = ten.Cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "dataset = 'nyyellowtaxi'\n",
    "local_dir=f'./{dataset}/'\n",
    "if not os.path.exists(local_dir):\n",
    "    print(f'Downloading dataset into {local_dir} ...')\n",
    "    snapshot_download(\n",
    "        repo_id=\"hkverma/\"+dataset,\n",
    "        repo_type=\"dataset\",\n",
    "        local_dir=local_dir,\n",
    "        local_dir_use_symlinks=False  # ensures real copies, not symlinks\n",
    "    )\n",
    "    print('Download Complete.')\n",
    "else:\n",
    "    print(f\"Dataset already exists in {local_dir}, skipping download.\")  \n",
    "nytaxi_data_dir=local_dir\n",
    "os.listdir(nytaxi_data_dir)\n",
    "nytaxi_train_file = nytaxi_data_dir+'train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_schema = pa.csv.ConvertOptions(\n",
    "    column_types={\n",
    "        'fare_amount': 'float32',\n",
    "        'pickup_datetime': 'string',\n",
    "        'pickup_longitude': 'float32',\n",
    "        'pickup_latitude': 'float32',\n",
    "        'dropoff_longitude': 'float32',\n",
    "        'dropoff_latitude': 'float32',\n",
    "        'passenger_count': 'uint8'\n",
    "    }\n",
    ")\n",
    "train_pa = pa.csv.read_csv(input_file=nytaxi_train_file, convert_options=convert_schema)\n",
    "table_name = tc.add_table(\"nyt_train\", train_pa, tc.FactTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create val pandas df for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size=100\n",
    "dtypes = {'fare_amount': 'float32',\n",
    "          'pickup_datetime': 'str',\n",
    "          'pickup_longitude': 'float32',\n",
    "          'pickup_latitude': 'float32',\n",
    "          'dropoff_longitude': 'float32',\n",
    "          'dropoff_latitude': 'float32',\n",
    "          'passenger_count': 'uint8'}\n",
    "\n",
    "input_file = nytaxi_data_dir+'train.csv'\n",
    "   \n",
    "val_df = tc.slice(table_name=\"nyt_train\", offset=0,length=val_size).to_pandas()\n",
    "val_df = remove_outliers(val_df)\n",
    "val_df = extract_features(val_df)\n",
    "val_df = val_df.drop(columns='key')\n",
    "\n",
    "X_val = val_df.drop(columns='fare_amount')\n",
    "y_val = val_df[['fare_amount']]\n",
    "\n",
    "dval = xgb.DMatrix(X_val, y_val, feature_names=X_val.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch training \n",
    "Read batched data using Liten data tensor slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'learning_rate': 0.05,\n",
    "          'max_depth': 7,\n",
    "          'objective': 'reg:linear',\n",
    "          'eval_metric': 'rmse',\n",
    "          'subsample': 0.8,\n",
    "          'gamma': 1,\n",
    "          'silent': True,\n",
    "          'verbose_eval': True}\n",
    "num_rounds = 16\n",
    "model = None\n",
    "batch_size = 1000\n",
    "val_size = 100\n",
    "    \n",
    "remaining_rows = train_pa.num_rows - val_size\n",
    "offset = val_size\n",
    "\n",
    "while (remaining_rows > 0):\n",
    "    batch_df = tc.slice(table_name=\"nyt_train\", offset=offset, length=batch_size).to_pandas()\n",
    "    remaining_rows -= batch_size\n",
    "    offset += batch_size\n",
    "    \n",
    "    batch_df = remove_outliers(batch_df)\n",
    "    batch_df = extract_features(batch_df)\n",
    "\n",
    "    batch_df = batch_df.drop(columns='key')\n",
    "    X_train = batch_df.drop(columns='fare_amount')\n",
    "    y_train = batch_df[['fare_amount']]\n",
    "    dtrain = xgb.DMatrix(X_train, y_train, feature_names=X_train.columns.tolist())\n",
    "    \n",
    "    model = xgb.train(params, dtrain, num_rounds, early_stopping_rounds=5,\n",
    "                      evals=[(dtrain, 'train'), (dval, 'eval')],\n",
    "                      xgb_model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b34818b9da74110900d2123bc68763375fbc09f3"
   },
   "outputs": [],
   "source": [
    "xgb.plot_importance(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a70ed21b43d720282bbae70e934b1188be2bc382"
   },
   "source": [
    "## Predictions\n",
    "Read test data from liten, add predicted columns. Prediction is to be added as Liten method as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nytaxi_test_file = nytaxi_data_dir+'test.csv'\n",
    "convert_schema = pa.csv.ConvertOptions(\n",
    "    column_types={\n",
    "        'key': 'string',\n",
    "        'pickup_datetime': 'string',\n",
    "        'pickup_longitude': 'float32',\n",
    "        'pickup_latitude': 'float32',\n",
    "        'dropoff_longitude': 'float32',\n",
    "        'dropoff_latitude': 'float32',\n",
    "        'passenger_count': 'uint8'\n",
    "    }\n",
    ")\n",
    "test_pa = pa.csv.read_csv(input_file=nytaxi_test_file, convert_options=convert_schema)\n",
    "table_name = tc.add_table(\"nyt_test\", test_pa, tc.FactTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3cbf4836cf8c71dfb67d13a9621b18a8d487197e"
   },
   "outputs": [],
   "source": [
    "test_pa = tc.slice(table_name=\"nyt_test\", offset=0, length=20)\n",
    "test_df = test_pa.to_pandas()\n",
    "test_df = extract_features(test_df)\n",
    "test_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_df.drop(columns='key')\n",
    "dtest = xgb.DMatrix(X_test, feature_names=X_test.columns.tolist())\n",
    "y_pred = model.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_arr = pa.array(y_pred)\n",
    "test_pa = test_pa.append_column('predicted_fare_amount', pred_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ddba4a856ff617411a641dfdf7635e47f969dff8"
   },
   "outputs": [],
   "source": [
    "test_pa.to_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
