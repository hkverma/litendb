{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TPCH Query 6 and Query 5 runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.range(1000).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "#tpch_dataset = 'tpch-sfdot001g' # for sf=0.001g \n",
    "tpch_dataset = 'tpch-sf1g' # for sf=1g\n",
    "local_dir=f'./{tpch_dataset}/'\n",
    "if not os.path.exists(local_dir):\n",
    "    print(f'Downloading dataset into {local_dir} ...')\n",
    "    snapshot_download(\n",
    "        repo_id=\"hkverma/\"+tpch_dataset,\n",
    "        repo_type=\"dataset\",\n",
    "        local_dir=local_dir,\n",
    "        local_dir_use_symlinks=False  # ensures real copies, not symlinks\n",
    "    )\n",
    "    print('Download Complete.')\n",
    "else:\n",
    "    print(f\"Dataset already exists in {local_dir}, skipping download.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('L_ORDERKEY',IntegerType(),True),\n",
    "                     StructField('L_PARTKEY',IntegerType(),True),\n",
    "                     StructField('L_SUPPKEY',IntegerType(),True),\n",
    "                     StructField('L_LINENUMBER',IntegerType(),True),\n",
    "                     StructField('L_QUANTITY',IntegerType(),True),\n",
    "                     StructField('L_EXTENDEDPRICE',DoubleType(),True),\n",
    "                     StructField('L_DISCOUNT',DoubleType(),True),\n",
    "                     StructField('L_TAX',DoubleType(),True),\n",
    "                     StructField('L_RETURNFLAG',StringType(),True),\n",
    "                     StructField('L_LINESTATUS',StringType(),True),\n",
    "                     StructField('L_SHIPDATE',DateType(),True),\n",
    "                     StructField('L_COMMITDATE',DateType(),True),\n",
    "                     StructField('L_RECEIPTDATE',DateType(),True),\n",
    "                     StructField('L_SHIPINSTRUCT',StringType(),True),\n",
    "                     StructField('L_SHIPMODE',StringType(),True),\n",
    "                     StructField('L_COMMENT',StringType(),True)])\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineitemDf = spark.read.format('csv').options(header='true').options(delimiter='|').schema(schema).load(f\"{local_dir}/lineitem.csv\")\n",
    "lineitemDf.printSchema()\n",
    "lineitemDf.show(5)\n",
    "lineitemDf.createOrReplaceTempView(\"lineitem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlDf = spark.sql(\"select sum(l_extendedprice * l_discount) as revenue from lineitem \"\n",
    "                  \"where l_shipdate >= date '1997-01-01' \"\n",
    "                  \"and l_shipdate < date '1997-01-01' + interval '1' year \"\n",
    "                  \" and l_discount between 0.07 - 0.01 and 0.07 + 0.01 \"\n",
    "                  \" and l_quantity < 25;\")\n",
    "sqlDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following neews to be done for the sql code shown below. load customer, orders, lineitem, supplier, nation, region tables. Then run query 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDf = spark.read.format('csv').options(header='true').options(delimiter='|').options(inferSchema='true').load(f\"{local_dir}/customer.csv\")\n",
    "customerDf.printSchema()\n",
    "customerDf.show(5)\n",
    "customerDf.createOrReplaceTempView(\"customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersDf = spark.read.format('csv').options(header='true').options(delimiter='|').options(inferSchema='true').load(f\"{local_dir}/orders.csv\")\n",
    "ordersDf.printSchema()\n",
    "ordersDf.show(5)\n",
    "ordersDf.createOrReplaceTempView(\"orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supplierDf = spark.read.format('csv').options(header='true').options(delimiter='|').options(inferSchema='true').load(f\"{local_dir}/supplier.csv\")\n",
    "supplierDf.printSchema()\n",
    "supplierDf.show(5)\n",
    "supplierDf.createOrReplaceTempView(\"supplier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nationDf = spark.read.format('csv').options(header='true').options(delimiter='|').options(inferSchema='true').load(f\"{local_dir}/nation.csv\")\n",
    "nationDf.printSchema()\n",
    "nationDf.show(5)\n",
    "nationDf.createOrReplaceTempView(\"nation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regionDf = spark.read.format('csv').options(header='true').options(delimiter='|').options(inferSchema='true').load(f\"{local_dir}/region.csv\")\n",
    "regionDf.printSchema()\n",
    "regionDf.show(5)\n",
    "regionDf.createOrReplaceTempView(\"region\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql5Df = spark.sql(\"select n_name, sum(l_extendedprice * (1 - l_discount)) as revenue \"\n",
    "    \"from customer, orders, lineitem, supplier, nation, region \"\n",
    "    \"where c_custkey = o_custkey \"\n",
    "    \"and l_orderkey = o_orderkey \"\n",
    "    \"and l_suppkey = s_suppkey \"\n",
    "    \"and c_nationkey = s_nationkey \"\n",
    "    \"and s_nationkey = n_nationkey \"\n",
    "    \"and n_regionkey = r_regionkey \"\n",
    "    \"and r_name = 'EUROPE' \"\n",
    "    \"and o_orderdate >= date '1995-01-01' \"\n",
    "    \"and o_orderdate < date '1995-01-01' + interval '1' year \"\n",
    "    \"group by n_name order by revenue desc\")\n",
    "sql5Df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
