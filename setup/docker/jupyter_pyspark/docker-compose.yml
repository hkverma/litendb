# Uses conda and package sizes are much bigger avoid using pyspark package for now
# docker compose --file docker-compose up
version: '3'
networks:
  pyspark_frontend:
    driver: bridge
services:
# jupyterlab with pyspark
  pyspark-notebook:
    image: jupyter/pyspark-notebook:spark-3.3.2
    networks:
     - pyspark_frontend
    ports:
      - "8888:8888"
      - "8080:8080"
      - "7077:7077"
      - "4040:4040"
    volumes:
      - /home/hkverma/work:/home/jovyan/work
    environment:
      - JUPYTER_ENABLE_LAB=yes
