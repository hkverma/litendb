# Uses conda and package sizes are much bigger avoid using pyspark package for now
# docker compose --file docker-compose-pyspark.yml up
version: '3'
networks:
  pyspark_frontend:
    driver: bridge
services:
# jupyterlab with pyspark
  pyspark-notebook:
    image: litendata/spark-py:v3.3.2
    networks:
     - pyspark_frontend
    ports:
      - "9889:8888"
      - "9081:8080"
      - "9078:7077"
      - "9040:4040"
    volumes:
      - ${WORKDIR}:/opt/spark/
    environment:
      - JUPYTER_ENABLE_LAB=yes
